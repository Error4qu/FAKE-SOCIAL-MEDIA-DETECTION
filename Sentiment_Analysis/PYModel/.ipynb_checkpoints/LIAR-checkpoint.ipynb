{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f278a1b2-d0ab-4508-a88a-832d23ebf644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                        articleBody\n",
      "0        0  A small meteorite crashed into a wooded area i...\n",
      "1        4  Last week we hinted at what was to come as Ebo...\n",
      "2        5  (NEWSER) â€“ Wonder how long a Quarter Pounder w...\n",
      "3        6  Posting photos of a gun-toting child online, I...\n",
      "4        7  At least 25 suspected Boko Haram insurgents we...\n",
      "                                            Headline  Body ID     Stance\n",
      "0  Police find mass graves with at least '15 bodi...      712  unrelated\n",
      "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
      "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
      "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
      "4  Spider burrowed through tourist's stomach and ...     1923   disagree\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_bodies = pd.read_csv(r\"C:\\Users\\91875\\Desktop\\smart\\Fake-SocialMedia-Detection\\Sentiment_Analysis\\Data_S\\Liar\\train_bodies.csv\")\n",
    "train_stances = pd.read_csv(r\"C:\\Users\\91875\\Desktop\\smart\\Fake-SocialMedia-Detection\\Sentiment_Analysis\\Data_S\\Liar\\train_stances.csv\")\n",
    "test_bodies = pd.read_csv(r\"C:\\Users\\91875\\Desktop\\smart\\Fake-SocialMedia-Detection\\Sentiment_Analysis\\Data_S\\Liar\\competition_test_bodies.csv\")\n",
    "test_stances = pd.read_csv(r\"C:\\Users\\91875\\Desktop\\smart\\Fake-SocialMedia-Detection\\Sentiment_Analysis\\Data_S\\Liar\\train_stances.csv\")\n",
    "\n",
    "# Take a look at the training data\n",
    "print(train_bodies.head())\n",
    "print(train_stances.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7adc6784-4c01-406a-8b51-f3d8d1b6abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Headline  Body ID     Stance  \\\n",
      "0  Police find mass graves with at least '15 bodi...      712  unrelated   \n",
      "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree   \n",
      "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated   \n",
      "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated   \n",
      "4  Spider burrowed through tourist's stomach and ...     1923   disagree   \n",
      "\n",
      "                                         articleBody  \n",
      "0  Danny Boyle is directing the untitled film\\n\\n...  \n",
      "1  Hundreds of Palestinians were evacuated from t...  \n",
      "2  30-year-old Moscow resident was hospitalized w...  \n",
      "3  (Reuters) - A Canadian soldier was shot at the...  \n",
      "4  Fear not arachnophobes, the story of Bunbury's...  \n"
     ]
    }
   ],
   "source": [
    "# Merge the training stances with the training bodies on 'Body ID'\n",
    "train_data = pd.merge(train_stances, train_bodies, on='Body ID')\n",
    "\n",
    "# Similarly, merge the test stances with the test bodies\n",
    "test_data = pd.merge(test_stances, test_bodies, on='Body ID')\n",
    "\n",
    "# Take a look at the merged data\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3746408f-82be-496d-b7cc-a46bfe97c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train_data: Headline       0\n",
      "Body ID        0\n",
      "Stance         0\n",
      "articleBody    0\n",
      "dtype: int64\n",
      "Missing values in test_data: Headline       0\n",
      "Body ID        0\n",
      "Stance         0\n",
      "articleBody    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 5000)) while a minimum of 1 is required by TfidfTransformer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Fit TF-IDF on the training data and transform both train and test sets\u001b[39;00m\n\u001b[0;32m     38\u001b[0m X_train \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 39\u001b[0m X_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Split training data into train/validation sets if needed\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Print shapes to verify everything is correct\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of X_train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2116\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2113\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2115\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[1;32m-> 2116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1688\u001b[0m, in \u001b[0;36mTfidfTransformer.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform a count matrix to a tf or tf-idf representation.\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \n\u001b[0;32m   1673\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1688\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1689\u001b[0m     X,\n\u001b[0;32m   1690\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1691\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[0;32m   1692\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1693\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1694\u001b[0m )\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1696\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1091\u001b[0m         )\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 5000)) while a minimum of 1 is required by TfidfTransformer."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you've already loaded the datasets into these variables:\n",
    "# train_stances, train_bodies, test_stances, test_bodies\n",
    "\n",
    "# Merge stances with bodies for training data\n",
    "train_data = pd.merge(train_stances, train_bodies, on='Body ID', how='inner')\n",
    "test_data = pd.merge(test_stances, test_bodies, on='Body ID', how='inner')\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in train_data: {train_data.isnull().sum()}\")\n",
    "print(f\"Missing values in test_data: {test_data.isnull().sum()}\")\n",
    "\n",
    "# Concatenate Headline and articleBody into a single text field for training and test sets\n",
    "train_data['text'] = train_data['Headline'].fillna('') + \" \" + train_data['articleBody'].fillna('')\n",
    "test_data['text'] = test_data['Headline'].fillna('') + \" \" + test_data['articleBody'].fillna('')\n",
    "\n",
    "# Remove rows with empty text fields if any\n",
    "train_data = train_data[train_data['text'].str.strip() != '']\n",
    "test_data = test_data[test_data['text'].str.strip() != '']\n",
    "\n",
    "# Preprocess the labels (assuming 'Stance' is the target label)\n",
    "# Example: Converting categorical labels to numerical form\n",
    "label_mapping = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
    "train_data['Stance'] = train_data['Stance'].map(label_mapping)\n",
    "test_data['Stance'] = test_data['Stance'].map(label_mapping)\n",
    "\n",
    "# Extract the labels for training and test sets\n",
    "y_train = train_data['Stance'].values\n",
    "y_test = test_data['Stance'].values\n",
    "\n",
    "# Text vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# Fit TF-IDF on the training data and transform both train and test sets\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Split training data into train/validation sets if needed\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes to verify everything is correct\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d728df-d39e-4e66-95d0-fa256f15af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacc46e-8747-4e46-848a-270a9445db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unlabeled test data\n",
    "unlabeled_bodies = pd.read_csv('unlabeled_test_bodies.csv')\n",
    "unlabeled_stances = pd.read_csv('test_stances_unlabeled.csv')\n",
    "\n",
    "# Merge the unlabeled stances with the bodies\n",
    "unlabeled_data = pd.merge(unlabeled_stances, unlabeled_bodies, on='Body ID')\n",
    "\n",
    "# Concatenate headlines and bodies for unlabeled data\n",
    "unlabeled_data['text'] = unlabeled_data['Headline'] + \" \" + unlabeled_data['articleBody']\n",
    "\n",
    "# Vectorize the unlabeled data using the same vectorizer\n",
    "X_unlabeled_vec = vectorizer.transform(unlabeled_data['text'])\n",
    "\n",
    "# Predict stances for unlabeled data\n",
    "unlabeled_predictions = model.predict(X_unlabeled_vec)\n",
    "\n",
    "# Output the predictions\n",
    "print(unlabeled_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571e3b3-f0e7-4eae-ab3c-c31402624752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
