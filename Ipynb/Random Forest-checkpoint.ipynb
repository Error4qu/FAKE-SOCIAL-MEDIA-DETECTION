{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881e45c8",
   "metadata": {},
   "source": [
    "## Detect fake profiles in online social networks using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2bbdde-c1f7-4b58-90f4-81a86d98728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "420ff138-a8b7-46ef-a61b-5e1a80caefff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading datasets...\n",
      "\n",
      "Extracting features...\n",
      "\n",
      "Index(['statuses_count', 'followers_count', 'friends_count',\n",
      "       'favourites_count', 'listed_count', 'lang_code'],\n",
      "      dtype='object')\n",
      "       statuses_count  followers_count  friends_count  favourites_count  \\\n",
      "count     2818.000000      2818.000000    2818.000000       2818.000000   \n",
      "mean      1672.198368       371.105039     395.363023        234.541164   \n",
      "std       4884.669157      8022.631339     465.694322       1445.847248   \n",
      "min          0.000000         0.000000       0.000000          0.000000   \n",
      "25%         35.000000        17.000000     168.000000          0.000000   \n",
      "50%         77.000000        26.000000     306.000000          0.000000   \n",
      "75%       1087.750000       111.000000     519.000000         37.000000   \n",
      "max      79876.000000    408372.000000   12773.000000      44349.000000   \n",
      "\n",
      "       listed_count    lang_code  \n",
      "count   2818.000000  2818.000000  \n",
      "mean       2.818666     2.851313  \n",
      "std       23.480430     1.992950  \n",
      "min        0.000000     0.000000  \n",
      "25%        0.000000     1.000000  \n",
      "50%        0.000000     1.000000  \n",
      "75%        1.000000     5.000000  \n",
      "max      744.000000     7.000000  \n",
      "Splitting datasets into train and validation...\n",
      "\n",
      "Training the model...\n",
      "\n",
      "The trained model is: RandomForestClassifier(n_estimators=40, oob_score=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### function for reading dataset from csv files\n",
    "\n",
    "def read_datasets():\n",
    "    \"\"\"Reads users profile from csv files\"\"\"\n",
    "    genuine_users = pd.read_csv(\"data/users.csv\")\n",
    "    fake_users = pd.read_csv(\"data/fusers.csv\")\n",
    "    x = pd.concat([genuine_users, fake_users])\n",
    "    y = len(fake_users) * [0] + len(genuine_users) * [1]\n",
    "    return x, y\n",
    "\n",
    "###### function for feature engineering\n",
    "\n",
    "def extract_features(x):\n",
    "    lang_list = list(enumerate(np.unique(x['lang'])))\n",
    "    lang_dict = {name: i for i, name in lang_list}\n",
    "    x.loc[:, 'lang_code'] = x['lang'].map(lambda x: lang_dict[x]).astype(int)\n",
    "    \n",
    "    feature_columns_to_use = ['statuses_count', 'followers_count', 'friends_count', \n",
    "                              'favourites_count', 'listed_count', 'lang_code']\n",
    "    x = x.loc[:, feature_columns_to_use]\n",
    "    return x\n",
    "\n",
    "###### function for training model\n",
    "\n",
    "def train_model(X_train, y_train, model_type='random_forest'):\n",
    "    \"\"\"Train the dataset using RandomForest or GradientBoosting.\"\"\"\n",
    "    \n",
    "    if model_type == 'random_forest':\n",
    "        clf = RandomForestClassifier(n_estimators=40, oob_score=True)\n",
    "    elif model_type == 'gradient_boosting':\n",
    "        clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"The trained model is: {clf}\")\n",
    "    return clf\n",
    "\n",
    "###### Main flow of the program\n",
    "\n",
    "print(\"Reading datasets...\\n\")\n",
    "x, y = read_datasets()\n",
    "x.describe()\n",
    "\n",
    "print(\"Extracting features...\\n\")\n",
    "x = extract_features(x)\n",
    "print(x.columns)\n",
    "print(x.describe())\n",
    "\n",
    "print(\"Splitting datasets into train and validation...\\n\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.20, random_state=44)\n",
    "\n",
    "print(\"Training the model...\\n\")\n",
    "trained_model = train_model(X_train, y_train, model_type='random_forest')\n",
    "\n",
    "# Now you can use this trained model in the `predict_n_rows` function to predict on input.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9124fe6a-79d3-4989-ac60-a9d9c56d6ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for 1 rows saved to C:\\Users\\91875\\Desktop\\smart\\Fake-SocialMedia-Detection\\Datatransition\\output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Function to read n rows from input file\n",
    "def read_n_rows(input_file, n):\n",
    "    input_data = pd.read_csv(input_file, nrows=n)\n",
    "    return input_data\n",
    "\n",
    "# Function to predict for n rows and write to output.csv\n",
    "def predict_n_rows(model, input_file, output_file, n):\n",
    "    # Load the input data\n",
    "    input_data = read_n_rows(input_file, n)\n",
    "    \n",
    "    # Extract features from input\n",
    "    input_data = extract_features(input_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(input_data)\n",
    "    \n",
    "    # Convert to class labels (0 or 1)\n",
    "    predicted_classes = predictions\n",
    "    \n",
    "    # Save predictions to output.csv\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Prediction'])  # Add header\n",
    "        for pred in predicted_classes:\n",
    "            writer.writerow([pred])\n",
    "    \n",
    "    print(f\"Predictions for {n} rows saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = r\"C:\\Users\\91875\\Desktop\\smart\\Fake-SocialMedia-Detection\\Datatransition\\input.csv\"\n",
    "output_file = r\"C:\\Users\\91875\\Desktop\\smart\\Fake-SocialMedia-Detection\\Datatransition\\output.csv\"\n",
    "n = 1  # Specify the number of rows to predict\n",
    "predict_n_rows(trained_model, input_file, output_file, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107d26f-effc-420a-aae4-1d07f2c4afba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
